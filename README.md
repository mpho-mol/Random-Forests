Random Forests is build up of decision trees as it combines the decisions from multiple individual decision trees to enhance predictive accuracy and muting noisy data and outliers which would otherwise cause overfitting in single decision trees. They are an ensemble learning algorithm used in machine learning for both classification and regression tasks. The notion behind ensemble learning is that by merging the knowledge of multiple models, the ensemble can often perform better than any single model. Estimates of feature importance are provided based on how often a feature is used for splitting in the trees. Like single decision trees, random forests have hyperparameters that can be tuned to improve performance. Since each decision tree can be trained independently, random forests can be parallelised so they are suitable for large-scale data.

The task was completed in Jupyter Notebook using the data in the titanic.csv file and is a continuation of the Decision Trees assignment. A bootstrap aggregation, random forest and boosted tree were created for the data set. From the random forest model the features which contributed the most to predicting the survival of passengers were determined. A list of values to try for n_estimators and max_depth was defined and a for loop over all of the possible combinations of n_estimators and max_depth to find the best model was created. Finally, the accuracy of all of the models and the model which performed the best were reported.
